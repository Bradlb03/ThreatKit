# threatkit/malware/routes.py

from flask import (
    Blueprint,
    render_template,
    request,
    redirect,
    url_for,
    flash,
)
import os
import pandas as pd

from . import ML_MODEL, SCALER
from .feats import extract_features_from_bytes, FEATURE_ORDER

# -----------------------------
# Blueprint + base paths
# -----------------------------
bp = Blueprint("malware", __name__, template_folder="../templates")

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
RULES_DIR = os.path.join(BASE_DIR, "rules")
YARA_RULES_PATH = os.path.join(RULES_DIR, "initial_rules.yar")

ALLOWED_EXTENSIONS = {"exe", "dll", "scr", "pdf", "zip", "docx", "xlsx", "txt"}

# -----------------------------
# Load YARA
# -----------------------------
try:
    import yara
    print("[DEBUG] Imported yara successfully")

    print("[DEBUG] Compiling rules from:", YARA_RULES_PATH)
    YARA_RULES = yara.compile(filepath=YARA_RULES_PATH)
    YARA_AVAILABLE = True
    print("[YARA] Loaded rules successfully")

except Exception as e:
    print("[YARA] FAILED to load rules or library:", e)
    yara = None
    YARA_RULES = None
    YARA_AVAILABLE = False


# -----------------------------
# Helpers
# -----------------------------
def allowed_file(filename: str) -> bool:
    return "." in filename and filename.rsplit(".", 1)[1].lower() in ALLOWED_EXTENSIONS


def run_yara_rules(data: bytes):
    """
    Run YARA rules on raw file bytes.
    Returns: (yara_score: float 0-1, rules_triggered: list[str])
    """
    if not YARA_AVAILABLE or YARA_RULES is None:
        return 0.0, []

    matches = YARA_RULES.match(data=data)
    rule_names = [m.rule for m in matches]

    if not matches:
        score = 0.0
    else:
        # Simple heuristic: one rule = 0.6, more rules push toward 1.0
        base = 0.6
        score = min(1.0, base + 0.1 * (len(matches) - 1))

    return float(score), rule_names


# -----------------------------
# Page routes (HTML)
# -----------------------------
@bp.route("/")
def index():
    # render the page; uploads go to /malware/scan
    return render_template("malware.html", result=None, filename=None)


# -----------------------------
# Core API: /malware/scan
# -----------------------------
@bp.route("/scan", methods=["POST"])
def scan_file():
    """
    Core malware scan endpoint.
    - Reads uploaded file bytes
    - Extracts static features (entropy, import_count)
    - Scales & runs ML model
    - Runs YARA rules
    - Combines both into a final decision
    - Returns a 1–5 score where 5 = safest and 1 = most dangerous
    """
    file = request.files.get("file")
    if not file or file.filename == "":
        flash("No file provided.")
        return redirect(url_for("malware.index"))

    if not allowed_file(file.filename):
        flash("Unsupported file type.")
        return redirect(url_for("malware.index"))

    data = file.read()
    if not data:
        flash("Empty file.")
        return redirect(url_for("malware.index"))

    # ---------- FEATURE EXTRACTION ----------
    feature_dict = extract_features_from_bytes(data)
    feature_order = FEATURE_ORDER

    X = pd.DataFrame(
        [[feature_dict[name] for name in feature_order]],
        columns=feature_order,
    )

    print("[DEBUG] Features:", feature_dict)  # DEBUG

    # ---------- SCALE + ML PREDICTION ----------
    ml_score = None

    # Scale if we have a scaler
    if SCALER is not None:
        try:
            X_scaled_array = SCALER.transform(X)
            X_scaled = pd.DataFrame(X_scaled_array, columns=feature_order)
        except Exception as e:
            print("[ML] Scaling error, using raw features:", e)
            X_scaled = X
    else:
        print("[ML] No scaler loaded; using raw features")
        X_scaled = X

    # Predict if we have a model
    if ML_MODEL is not None:
        try:
            # IMPORTANT: this assumes class 1 = malicious.
            # If your training labels use class 0 = malicious, change [1] to [0].
            proba = ML_MODEL.predict_proba(X_scaled)[0][1]  # P(malicious)
            ml_score = float(proba)
        except Exception as e:
            print("[ML] Predict error:", e)
            ml_score = None
    else:
        print("[ML] No ML model loaded")
        ml_score = None

    # If ML failed, treat as medium risk instead of "definitely safe"
    if ml_score is None:
        ml_score = 0.5

    # ---------- YARA ----------
    yara_score, rules_triggered = run_yara_rules(data)

    # ---------- COMBINE ----------
    # Combined "maliciousness" score: 0 = benign, 1 = very malicious
    w_ml = 0.6
    w_yara = 0.4
    combined_score = w_ml * ml_score + w_yara * yara_score

    # Strong YARA hit overrides to malicious
    if rules_triggered and yara_score >= 0.6:
        final_label = "malicious"
    else:
        threshold = 0.4  # tune if needed
        final_label = "malicious" if combined_score >= threshold else "benign"

    # ---------- MAP TO 1–5 SCORE (5 = safest, 1 = most dangerous) ----------
    # Start from combined_score as "maliciousness" (0–1).
    risk_raw = combined_score

    # If we’ve decided it’s malicious, force at least medium–high risk
    if final_label == "malicious":
        risk_raw = max(risk_raw, 0.75)

    # Convert maliciousness (0–1) to safety score 1–5:
    # - risk_raw = 0   -> score_5 = 5 (Excellent / very safe)
    # - risk_raw = 1   -> score_5 = 1 (Dangerous)
    score_5 = max(1, min(5, int(round((1 - risk_raw) * 4)) + 1))

    score_levels = {
        1: ("Dangerous", "text-danger"),
        2: ("Suspicious", "text-danger"),
        3: ("Moderate", "text-warning"),
        4: ("Good", "text-success"),
        5: ("Excellent", "text-success"),
    }
    score_label, score_color = score_levels[score_5]

    # ---------- DEBUG LINE ----------
    print(
        f"[DEBUG] ml={ml_score:.3f}, yara={yara_score:.3f}, "
        f"combined={combined_score:.3f}, final_label={final_label}, "
        f"risk_raw={risk_raw:.3f}, score_5={score_5}"
    )

    # ---------- RENDER ----------
    result = {
        "prediction": final_label.title(),
        "combined": round(combined_score, 3),
        "ml": round(ml_score, 3),
        "yara": round(yara_score, 3),
        "rules": rules_triggered,
        "score_5": score_5,
        "score_label": score_label,
        "score_color": score_color,
    }

    return render_template("malware.html", result=result, filename=file.filename)
