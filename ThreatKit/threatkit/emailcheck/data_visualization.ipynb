{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Load evaluation results from ThreatKit offline eval\n",
    "results_path = \"threatkit/emailcheck/eval/eval_results.csv\"\n",
    "df = pd.read_csv(results_path)\n",
    "\n",
    "# Basic sanity check\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "phish_scores = df[df[\"label\"] == 1][\"score_0_5\"]\n",
    "safe_scores  = df[df[\"label\"] == 0][\"score_0_5\"]\n",
    "\n",
    "plt.hist(safe_scores, bins=20, alpha=0.5, label=\"Safe (label=0)\")\n",
    "plt.hist(phish_scores, bins=20, alpha=0.5, label=\"Phishing (label=1)\")\n",
    "\n",
    "plt.xlabel(\"Safe score (0 = risky, 5 = safe)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of ThreatKit safe scores by class\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_true = df[\"label\"].values\n",
    "y_pred = df[\"pred_phish\"].values\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred)\n",
    "rec = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(\"Accuracy :\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall   :\", rec)\n",
    "print(\"F1-score :\", f1)\n",
    "print()\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Safe (0)\", \"Phish (1)\"])\n",
    "disp.plot()\n",
    "plt.title(\"Confusion Matrix – ThreatKit Phishing Detector\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate phishing probability from safe score\n",
    "safe = df[\"score_0_5\"].values\n",
    "p_phish = 1.0 - (safe / 5.0)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, p_phish)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(fpr, tpr, lw=2, label=f\"ROC curve (AUC = {roc_auc:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=1, label=\"Random\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "plt.title(\"ROC Curve – ThreatKit Phishing Detector\")\n",
    "plt.legend()\n",
    "plt.axis(\"square\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_thresholds(scores, labels, thresholds):\n",
    "    rows = []\n",
    "    for t in thresholds:\n",
    "        preds = (scores < t).astype(int)  # phishing if safe_score < t\n",
    "        rows.append({\n",
    "            \"threshold\": t,\n",
    "            \"accuracy\": accuracy_score(labels, preds),\n",
    "            \"precision\": precision_score(labels, preds, zero_division=0),\n",
    "            \"recall\": recall_score(labels, preds, zero_division=0),\n",
    "            \"f1\": f1_score(labels, preds, zero_division=0),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "thresholds = np.arange(2.0, 4.6, 0.25)\n",
    "sweep_df = evaluate_thresholds(df[\"score_0_5\"].values, y_true, thresholds)\n",
    "sweep_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(sweep_df[\"threshold\"], sweep_df[\"accuracy\"], marker=\"o\", label=\"Accuracy\")\n",
    "plt.plot(sweep_df[\"threshold\"], sweep_df[\"precision\"], marker=\"o\", label=\"Precision\")\n",
    "plt.plot(sweep_df[\"threshold\"], sweep_df[\"recall\"], marker=\"o\", label=\"Recall\")\n",
    "plt.plot(sweep_df[\"threshold\"], sweep_df[\"f1\"], marker=\"o\", label=\"F1-score\")\n",
    "\n",
    "plt.axvline(4.0, linestyle=\"--\", label=\"Chosen threshold (4.0)\")\n",
    "\n",
    "plt.xlabel(\"Safe score threshold t (score < t ⇒ phishing)\")\n",
    "plt.ylabel(\"Metric value\")\n",
    "plt.title(\"Metric trade-offs vs threshold\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
